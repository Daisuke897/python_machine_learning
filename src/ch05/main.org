次元削減でデータを圧縮する

#+begin_src emacs-lisp
  (pyvenv-activate "~/project/python_machine_learning")
#+end_src

#+RESULTS:

* 主成分分析による教師なし次元削減

** 主成分を抽出する

Wineデータ・セットを読み込む
#+begin_src python :session :results value
  import numpy as np
  import pandas as pd

  df_wine = pd.read_csv(
      'https://archive.ics.uci.edu/'
      'ml/machine-learning-databases/wine/wine.data',
      header=None
  )
  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                     'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                     'Proline']


  print('Class labels', np.unique(df_wine['Class label']))
  df_wine.head()
#+end_src

#+RESULTS:
:    Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  ...  Proanthocyanins  Color intensity   Hue  OD280/OD315 of diluted wines  Proline
: 0            1    14.23        1.71  2.43               15.6  ...             2.29             5.64  1.04                          3.92     1065
: 1            1    13.20        1.78  2.14               11.2  ...             1.28             4.38  1.05                          3.40     1050
: 2            1    13.16        2.36  2.67               18.6  ...             2.81             5.68  1.03                          3.17     1185
: 3            1    14.37        1.95  2.50               16.8  ...             2.18             7.80  0.86                          3.45     1480
: 4            1    13.24        2.59  2.87               21.0  ...             1.82             4.32  1.04                          2.93      735
: 
: [5 rows x 14 columns]

Wineデータセットを処理して訓練とテストに分割
分散が1となるように標準化

#+begin_src python :session :results output
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler

  X = df_wine.iloc[:, 1:].values
  y = df_wine.iloc[:, 0].values

  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size = 0.3, stratify = y, random_state = 0
  )

  sc = StandardScaler()
  X_train_std = sc.fit_transform(X_train)
  X_test_std = sc.transform(X_test)
#+end_src

#+RESULTS:

#+begin_src python :session :results output
  import numpy as np

  cov_mat = np.cov(X_train_std.T) # 共分散行列を作成
  eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) # 固有値と固有ベクトルを計算
  print("\nEigenvalues \n%s" % eigen_vals)
#+end_src

#+RESULTS:
: 
: Eigenvalues 
: [4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634
:  0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835
:  0.1808613 ]

固有値の分散説明率をプロットする

#+begin_src python :session :results file link
  # 固有値を合計
  tot = sum(eigen_vals)
  # 分散説明率を計算
  var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
  # 分散説明率の累積和を取得
  cum_var_exp = np.cumsum(var_exp)
  import matplotlib.pyplot as plt
  plt.close("all")
  # 分散説明率の棒グラフを作成
  plt.bar(range(1, 14), var_exp, alpha=0.5, align="center",
          label="Individual explained variance")
  # 分散説明率の累積和の階段グラフを作成
  plt.step(range(1, 14), cum_var_exp, where="mid",
           label="Cumulative explained variance")
  plt.ylabel("Explained variance ratio")
  plt.xlabel("Principal component index")
  plt.legend(loc="best")
  plt.tight_layout()

  fname = "images/05_02.png"

  plt.savefig(fname)

  fname
#+end_src

#+RESULTS:
[[file:images/05_02.png]]

固有値が大きいものから順に固有対を並び替え、選択された固有ベクトルから射影行列を生成する。
そして、この射影行列を使ってデータをより低い次元の部分空間に変換する。

#+begin_src python :session :results output
  # (固有値、固有ベクトル)のタプルのリストを作成
  eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]
  # (固有値、固有ベクトル)のタプルを大きいものから順に並び替え
  eigen_pairs.sort(key=lambda k: k[0], reverse=True)
#+end_src

#+RESULTS:

#+begin_src python :session :results value
  eigen_pairs
#+end_src

#+RESULTS:
|  4.8427453156558915 | array | ((-0.13724218 0.24724326 -0.02545159 0.20694508 -0.15436582 -0.39376952 -0.41735106 0.30572896 -0.30668347 0.07554066 -0.32613263 -0.36861022 -0.29669651))                 |
|  2.4160245870352237 | array | ((0.50303478 0.16487119 0.24456476 -0.11352904 0.28974518 0.05080104 -0.02287338 0.09048885 0.00835233 0.54977581 -0.20716433 -0.24902536 0.38022942))                      |
|  1.5484582488203522 | array | ((-0.13774873 0.09615039 0.67777567 0.62504055 0.19613548 0.14031057 0.11705386 0.13121778 0.0304309 -0.07992997 0.05305915 0.13239103 -0.07065022))                        |
|  0.9612043774977361 | array | ((-0.0032961 0.56264669 -0.10897711 0.0338187 -0.36751107 0.24024513 0.1870533 -0.02292622 0.49626233 0.10648294 -0.36905375 0.14201609 -0.16768217))                       |
|  0.8416616104578416 | array | ((-0.29062523 0.08953787 -0.16083499 0.05158734 0.67648707 -0.11851114 -0.10710035 -0.50758161 0.20163462 0.00573607 -0.27691422 -0.06662756 -0.12802904))                  |
|   0.662063404038305 | array | ((0.299096847 0.627036396 0.000389128239 -0.0405836452 0.0657772614 -0.0589776247 -0.030110318 -0.271728086 -0.439997519 -0.411743459 0.141673377 0.175842384 0.138018388)) |
|  0.5182847213561952 | array | ((0.07905293 -0.27400201 0.13232805 0.2239991 -0.40526897 -0.03474194 0.04178357 -0.63114569 -0.32312277 0.26908262 -0.30264066 0.13054014 0.00081134))                     |
|  0.3465037664128673 | array | ((-0.36817641 -0.01257758 0.17757818 -0.44059211 0.1166175 0.35019213 0.21871818 0.19712942 -0.43305587 -0.06684118 -0.45976229 0.11082755 0.00560817))                     |
| 0.31313680047208825 | array | ((-0.39837702 0.11045823 0.38249686 -0.24337385 -0.25898236 -0.34231286 -0.03612316 -0.17143688 0.24437021 -0.15551492 0.02119612 -0.23808956 0.51727846))                  |
| 0.21357214660527357 | array | ((0.37463888 -0.1374056 0.46158303 -0.41895399 0.01004706 -0.22125424 -0.04175136 -0.08875695 0.19992186 -0.22166887 -0.09846946 0.01912058 -0.54253207))                   |
| 0.18086130479496607 | array | ((0.26283426 -0.26676921 -0.11554255 0.19948341 0.02890188 -0.06638686 -0.21334908 0.18639128 0.16808299 -0.46636903 -0.53248388 0.23783528 0.36776336))                    |
| 0.15362835006711062 | array | ((-0.12783451 0.08064016 0.01679249 -0.11084566 0.07938796 -0.49145931 -0.0503074 0.17532803 -0.00367596 0.35975654 0.04046698 0.74222954 0.03873952))                      |
| 0.10754642369670948 | array | ((-0.09448698 0.02636524 0.14274751 -0.13048578 -0.06760808 0.45991766 -0.81458395 -0.09574809 0.06724689 0.08733362 0.12906113 0.18764627 0.01211126))                     |

13 x 2 の射影行列
#+begin_src python :session :results value
  w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))
  w
#+end_src

#+RESULTS:
| -0.13724218 |  0.50303478 |
|  0.24724326 |  0.16487119 |
| -0.02545159 |  0.24456476 |
|  0.20694508 | -0.11352904 |
| -0.15436582 |  0.28974518 |
| -0.39376952 |  0.05080104 |
| -0.41735106 | -0.02287338 |
|  0.30572896 |  0.09048885 |
| -0.30668347 |  0.00835233 |
|  0.07554066 |  0.54977581 |
| -0.32613263 | -0.20716433 |
| -0.36861022 | -0.24902536 |
| -0.29669651 |  0.38022942 |

#+begin_src python :session :results value
X_train_std.shape
#+end_src

#+RESULTS:
| 124 | 13 |

内積から2つの主成分に変換する
#+begin_src python :session :results value
  X_train_pca = X_train_std.dot(w)
  X_train_pca
#+end_src

#+RESULTS:
|  2.38299011 |  0.45458499 |
| -1.96578183 |  1.65376939 |
| -2.53907598 |  1.02909066 |
| -1.43010776 |   0.6024011 |
|  3.14147227 |  0.66214979 |
|  0.50253552 | -2.08907131 |
|  0.04867722 | -2.27536044 |
|  2.47888989 | -0.08603318 |
|  2.01900259 |  -1.3538719 |
|  0.75156583 | -2.55367947 |
|  0.72268915 | -1.18404391 |
| -3.00366211 |  0.94626934 |
|  2.57518878 |  -1.0697549 |
|  3.73151104 |  1.01968876 |
| -1.12276518 |     0.13877 |
|  2.85996853 |  2.28819559 |
| -0.74717125 | -3.21746061 |
| -1.58427878 |  0.16048055 |
|  3.38887101 |  2.11550689 |
|  3.15405473 |  0.54233966 |
| -1.28036506 | -1.72926871 |
| -1.71438911 |  0.71745249 |
| -1.55040291 |  -1.7580591 |
|  1.10984489 | -1.20480693 |
| -0.69108418 | -1.71385374 |
|   -2.086036 | -1.68453671 |
|  2.90393456 |  1.95258805 |
| -2.07635784 |  1.47183304 |
| -1.74756185 | -1.25842546 |
|  2.59424456 |  -0.1056037 |
| -2.50372355 |  0.70412212 |
| -2.19448402 |  2.18657552 |
|  3.91634534 |  0.16136475 |
| -1.11739618 |  0.51921086 |
| -0.89996804 | -2.04759575 |
| -1.71469178 |  0.61392169 |
| -2.48581303 |  0.76839561 |
| -0.76080562 | -1.67615627 |
|   2.9265371 |  0.18854741 |
|  2.94423716 |  1.34812388 |
| -2.38993219 |   1.0848074 |
|  2.63885049 |  0.75274937 |
|  2.51009031 |  2.25237953 |
|  3.65248086 |  1.74839925 |
| -2.65169609 |  1.01997476 |
|  0.52544559 | -2.13528249 |
|  2.70197573 |  0.56476307 |
|  3.18414708 |  2.58094695 |
|  1.12517041 | -1.85054449 |
|  2.92366519 |  0.41699915 |
| -1.96122314 | -1.28613661 |
|  0.54473673 | -1.07897226 |
| -0.77030308 | -1.93386815 |
| -1.16670455 |  0.00489815 |
| -1.36475309 | -2.13572269 |
|  0.43563732 | -2.56929607 |
|  2.96191745 |  1.91091009 |
|  2.83609557 |  0.65386032 |
|  1.90402089 | -0.35296542 |
|  -2.4858391 | -0.21308835 |
| -2.16575568 |   1.1468486 |
|  0.00669776 | -0.94337624 |
|  1.06560181 |  3.31221025 |
|  2.13117911 |  1.90551304 |
|  1.53543483 | -1.50854979 |
| -2.66783112 |  1.75933599 |
|  0.57279998 |  -2.7511383 |
| -0.70710916 | -2.43798549 |
| -0.99606577 |  -1.4772411 |
| -2.67324153 |  1.35779609 |
| -2.36367378 |  1.66537927 |
| -0.39171875 |  0.13747499 |
| -2.98908845 |  2.16983165 |
| -1.91822539 |  1.60141809 |
|   2.3114458 |    0.207123 |
| -1.06050503 |   0.6004608 |
| -2.74858609 | -0.29016054 |
|  2.26650077 |  2.14491758 |
| -1.15517469 | -0.50262909 |
|  0.16602503 | -2.26850051 |
|  1.35589389 |  0.33353007 |
| -3.31185057 |  1.39240115 |
| -0.33245686 | -2.15639865 |
| -2.23205085 |  0.52868143 |
|  0.18583758 | -1.44446967 |
|  0.84560856 |  0.17151684 |
|  2.69500472 |  2.74522492 |
|  0.44645674 | -0.62393943 |
| -1.88961007 | -0.04400723 |
| -3.08131761 |  1.59724429 |
| -3.45716348 |  1.21428442 |
|  3.87665629 |  0.46446004 |
|    1.575516 | -1.82299839 |
| -3.43344371 |   1.6116814 |
| -4.20642597 |  2.20145366 |
| -0.14042971 | -2.36871639 |
|  1.82731521 | -1.39485103 |
|  2.20564744 |  1.28462066 |
|  1.64999054 |  2.33211134 |
|  -1.4611033 | -0.46480324 |
| -0.60047516 |  0.00920072 |
| -3.08276231 |  0.28287148 |
|  0.45035749 | -2.20263755 |
|  0.90806897 |  -2.0881686 |
|  3.24973637 | -0.18273485 |
| -3.07882055 |  0.69622621 |
|  2.54277306 |  1.88571652 |
| -2.84838157 |  0.63274325 |
| -0.88997271 | -0.67927226 |
|  0.32368249 | -2.07006175 |
|  0.32007527 | -2.88708519 |
|  0.44889188 | -2.14872532 |
| -2.46582558 |   1.0745577 |
|  2.81678113 |  0.56344444 |
| -2.16983025 |  0.16644199 |
| -2.66728229 |  1.38137702 |
| -3.53223924 |  2.57906029 |
| -1.96637688 |  1.18319185 |
|  1.68741216 | -1.35075321 |
|  0.43521077 | -2.40355817 |
|  2.59045115 |  1.63852921 |
|  4.35308397 |  0.66536041 |
| -1.84315373 | -1.50688415 |
| -0.40860955 | -1.29720607 |

2次元の散布図としてプロットする
#+begin_src python :session :results file link
  colors = ["r", "b", "g"]
  markers = ["s", "x", "o"]

  plt.close("all")

  for l, c, m in zip(np.unique(y_train), colors, markers):
      plt.scatter(X_train_pca[y_train == l, 0],
                  X_train_pca[y_train == l, 1],
                  c = c,
                  label = l,
                  marker = m)

  plt.xlabel("PC 1")
  plt.ylabel("PC 2")
  plt.legend(loc="lower left")
  plt.tight_layout()

  fname = "images/05_03.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/05_03.png]]

scikit-learnによる主成分分析
#+begin_src python :session :results output
  from matplotlib.colors import ListedColormap
  import matplotlib.pyplot as plt

  def plot_decision_regions(
          X, y, classifier, resolution=0.02
  ):

      # setup marker generator and color map
      markers = ('s', 'x', 'o', '^', 'v')
      colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
      cmap = ListedColormap(colors[:len(np.unique(y))])

      # plot the decision surface
      x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
      x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                             np.arange(x2_min, x2_max, resolution))
      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
      Z = Z.reshape(xx1.shape)
      plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
      plt.xlim(xx1.min(), xx1.max())
      plt.ylim(xx2.min(), xx2.max())

      for idx, cl in enumerate(np.unique(y)):
          plt.scatter(x=X[y == cl, 0],
                      y=X[y == cl, 1],
                      alpha=0.8,
                      color=colors[idx],
                      marker=markers[idx],
                      label=cl,
                      edgecolor='black')

#+end_src

#+RESULTS:

#+begin_src python :session :results file link
  from sklearn.linear_model import LogisticRegression
  from sklearn.decomposition import PCA
  # 主成分数を指定して、PCAインスタンスを生成
  pca = PCA(n_components = 2)
  # ロジスティック回帰のインスタンスを生成
  lr = LogisticRegression(
      multi_class = "ovr",
      random_state = 1,
      solver = "lbfgs"
  )
  # 次元削減
  X_train_pca = pca.fit_transform(X_train_std)
  X_test_pca = pca.transform(X_test_std)
  # 削減したデータセットでロジスティック回帰モデルを適合
  lr.fit(X_train_pca, y_train)
  # 決定領域を描画
  plt.close("all")
  plot_decision_regions(X_train_pca, y_train, classifier = lr)
  plt.xlabel("PC1")
  plt.ylabel("PC2")
  plt.legend(loc = "lower left")
  plt.tight_layout()

  fname = "images/05_04.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/05_04.png]]

#+begin_src python :session :results file link
  plt.close("all")
  plot_decision_regions(X_test_pca, y_test, classifier = lr)
  plt.xlabel("PC1")
  plt.ylabel("PC2")
  plt.legend(loc = "lower left")
  plt.tight_layout()

  fname = "images/05_05.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/05_05.png]]

#+begin_src python :session :results value
  pca = PCA(n_components = None)
  X_train_pca = pca.fit_transform(X_train_std)
  # 分散説明率を計算
  pca.explained_variance_ratio_
#+end_src

#+RESULTS:
| 0.36951469 | 0.18434927 | 0.11815159 | 0.07334252 | 0.06422108 | 0.05051724 | 0.03954654 | 0.02643918 | 0.02389319 | 0.01629614 | 0.01380021 | 0.01172226 | 0.00820609 |

* 線形判別分析による教師ありデータ圧縮

平均ベクトルの計算
#+begin_src python :session :results output
  np.set_printoptions(precision=4)
  mean_vecs = []
  for label in range(1, 4):
      mean_vecs.append(np.mean(X_train_std[y_train==label], axis=0))
      print("MV %s: %s\n" %(label, mean_vecs[label-1]))
#+end_src

#+RESULTS:
: MV 1: [ 0.9066 -0.3497  0.3201 -0.7189  0.5056  0.8807  0.9589 -0.5516  0.5416
:   0.2338  0.5897  0.6563  1.2075]
: 
: MV 2: [-0.8749 -0.2848 -0.3735  0.3157 -0.3848 -0.0433  0.0635 -0.0946  0.0703
:  -0.8286  0.3144  0.3608 -0.7253]
: 
: MV 3: [ 0.1992  0.866   0.1682  0.4148 -0.0451 -1.0286 -1.2876  0.8287 -0.7795
:   0.9649 -1.209  -1.3622 -0.4013]

平均ベクトルを使ってクラス内変動行列を計算する

#+begin_src python :session :results output
  d = 13                          # 特徴量の個数
  S_W = np.zeros((d, d))
  for label, mv in zip(range(1, 4), mean_vecs):
      class_scatter = np.zeros((d, d))
      for row in X_train_std[y_train == label]:
          row, mv = row.reshape(d, 1), mv.reshape(d, 1)
          class_scatter += (row - mv).dot((row - mv).T)
      S_W += class_scatter

  print("Within-class scatter matrix: %sx%s" % (S_W.shape[0], S_W.shape[1]))
#+end_src

#+RESULTS:
: Within-class scatter matrix: 13x13

変動行列を計算するときは、訓練データセットにおいてクラスレベルが一様に分布していることが前提となる
しかし、クラスレベルの個数を出力するとこの前提を満たしていないことがわかる。

#+begin_src python :session :results output
  print("Class label distribution: %s" % np.bincount(y_train)[1:])
#+end_src

#+RESULTS:
: Class label distribution: [41 50 33]

スケーリングの必要がある。共分散行列の計算と同じ。
#+begin_src python :session :results output
  d = 13                          # 特徴量の個数
  S_W = np.zeros((d, d))
  for label, mv in zip(range(1, 4), mean_vecs):
      class_scatter = np.cov(X_train_std[y_train == label].T)
      S_W += class_scatter

  print("Scaled within-class scatter matrix: %sx%s" % (S_W.shape[0], S_W.shape[1]))
#+end_src

#+RESULTS:
: Scaled within-class scatter matrix: 13x13

クラス間変動行列の計算
#+begin_src python :session :results output
  mean_overall = np.mean(X_train_std, axis = 0)
  d = 13                          # 特徴量の個数
  S_B = np.zeros((d, d))
  for i, mean_vec in enumerate(mean_vecs):
      n = X_train_std[y_train == i + 1, :].shape[0]
      mean_vec = mean_vec.reshape(d, 1)
      mean_overall = mean_overall.reshape(d, 1)
      S_B += (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)

  print("Between-class scatter matrix: %sx%s" % (S_B.shape[0], S_B.shape[1]))
#+end_src

#+RESULTS:
: Between-class scatter matrix: 13x13
