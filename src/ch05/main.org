次元削減でデータを圧縮する

#+begin_src emacs-lisp
  (pyvenv-activate "~/project/python_machine_learning")
#+end_src

#+RESULTS:

* 主成分分析による教師なし次元削減

** 主成分を抽出する

Wineデータ・セットを読み込む
#+begin_src python :session :results value
  import numpy as np
  import pandas as pd

  df_wine = pd.read_csv(
      'https://archive.ics.uci.edu/'
      'ml/machine-learning-databases/wine/wine.data',
      header=None
  )
  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                     'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                     'Proline']


  print('Class labels', np.unique(df_wine['Class label']))
  df_wine.head()
#+end_src

#+RESULTS:
:    Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  ...  Proanthocyanins  Color intensity   Hue  OD280/OD315 of diluted wines  Proline
: 0            1    14.23        1.71  2.43               15.6  ...             2.29             5.64  1.04                          3.92     1065
: 1            1    13.20        1.78  2.14               11.2  ...             1.28             4.38  1.05                          3.40     1050
: 2            1    13.16        2.36  2.67               18.6  ...             2.81             5.68  1.03                          3.17     1185
: 3            1    14.37        1.95  2.50               16.8  ...             2.18             7.80  0.86                          3.45     1480
: 4            1    13.24        2.59  2.87               21.0  ...             1.82             4.32  1.04                          2.93      735
: 
: [5 rows x 14 columns]

Wineデータセットを処理して訓練とテストに分割
分散が1となるように標準化

#+begin_src python :session :results output
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler

  X = df_wine.iloc[:, 1:].values
  y = df_wine.iloc[:, 0].values

  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size = 0.3, stratify = y, random_state = 0
  )

  sc = StandardScaler()
  X_train_std = sc.fit_transform(X_train)
  X_test_std = sc.transform(X_test)
#+end_src

#+RESULTS:

#+begin_src python :session :results output
  import numpy as np

  cov_mat = np.cov(X_train_std.T) # 共分散行列を作成
  eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) # 固有値と固有ベクトルを計算
  print("\nEigenvalues \n%s" % eigen_vals)
#+end_src

#+RESULTS:
: 
: Eigenvalues 
: [4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634
:  0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835
:  0.1808613 ]

固有値の分散説明率をプロットする

#+begin_src python :session :results file link
  # 固有値を合計
  tot = sum(eigen_vals)
  # 分散説明率を計算
  var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
  # 分散説明率の累積和を取得
  cum_var_exp = np.cumsum(var_exp)
  import matplotlib.pyplot as plt
  plt.close("all")
  # 分散説明率の棒グラフを作成
  plt.bar(range(1, 14), var_exp, alpha=0.5, align="center",
          label="Individual explained variance")
  # 分散説明率の累積和の階段グラフを作成
  plt.step(range(1, 14), cum_var_exp, where="mid",
           label="Cumulative explained variance")
  plt.ylabel("Explained variance ratio")
  plt.xlabel("Principal component index")
  plt.legend(loc="best")
  plt.tight_layout()

  fname = "images/05_02.png"

  plt.savefig(fname)

  fname
#+end_src

#+RESULTS:
[[file:images/05_02.png]]

