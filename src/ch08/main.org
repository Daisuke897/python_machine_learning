*機械学習の適用1 感情分析*

#+begin_src emacs-lisp
  (pyvenv-activate "~/project/python_machine_learning")
#+end_src

#+RESULTS:

* IMDbの映画レビューデータセットでのテキスト処理

** 映画レビューデータセットを取得する
#+begin_src python :session :results value
  import pyprind
  import pandas as pd
  import os
  # "basepath"の値を展開した映画レビューデータセットのディレクトリに置き換える
  basepath = "aclImdb"
  labels = {"pos": 1, "neg": 0}
  pbar = pyprind.ProgBar(50000)
  df = pd.DataFrame()
  for s in ("test", "train"):
      for l in ("pos", "neg"):
          path = os.path.join(basepath, s, l)
          for file in sorted(os.listdir(path)):
              with open(os.path.join(path, file), "r", encoding = "utf-8") as infile:
                  txt = infile.read()
              df = df._append([[txt, labels[l]]], ignore_index = True)
              pbar.update()

  df.columns = ["review", "sentiment"]

#+end_src

#+RESULTS:

#+begin_src python :session :results output
  import numpy as np
  np.random.seed(0)
  df = df.reindex(np.random.permutation(df.index))
  df.to_csv("movie_data.csv", index = False, encoding = "utf-8")
#+end_src

#+RESULTS:

#+begin_src python :session :results value
  import pandas as pd
  df = pd.read_csv("movie_data.csv", encoding = "utf-8")
  df.head(3)
#+end_src

#+RESULTS:
:                                               review  sentiment
: 0  In 1974, the teenager Martha Moxley (Maggie Gr...          1
: 1  OK... so... I really like Kris Kristofferson a...          0
: 2  ***SPOILER*** Do not read this, if you think a...          0

1のクラスラベルは「肯定的」、0のラベルは「否定的」を表す

DataFrameオブジェクトが50,000行で構成されていることを確認
#+begin_src python :session :results value
  df.shape
#+end_src

#+RESULTS:
| 50000 | 2 |

* BoWモデルの紹介

** 単語を特徴量ベクトルに変換する
各文章に含まれる単語に基づいてBoWモデルを構築するには、scikit-learnに実装されているCountVectorizerクラスを利用できる。
#+begin_src python :session :results value
  import numpy as np
  from sklearn.feature_extraction.text import CountVectorizer
  count = CountVectorizer()
  docs = np.array([
      "The sun is shining",
      "The weather is sweet",
      "The sun is shining, the weather is sweet, and one and one is two",
  ])
  bag = count.fit_transform(docs)
  bag
#+end_src

#+RESULTS:
#+begin_example
<Compressed Sparse Row sparse matrix of dtype 'int64'
	with 17 stored elements and shape (3, 9)>
  Coords	Values
  (0, 6)	1
  (0, 4)	1
  (0, 1)	1
  (0, 3)	1
  (1, 6)	1
  (1, 1)	1
  (1, 8)	1
  (1, 5)	1
  (2, 6)	2
  (2, 4)	1
  (2, 1)	3
  (2, 3)	1
  (2, 8)	1
  (2, 5)	1
  (2, 0)	2
  (2, 2)	2
  (2, 7)	1
#+end_example

#+begin_src python :session :results output
  print(count.vocabulary_)
#+end_src

#+RESULTS:
: {'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}

#+begin_src python :session :results output
  print(bag.toarray())
#+end_src

#+RESULTS:
: [[0 1 0 1 1 0 1 0 0]
:  [0 1 0 0 0 1 1 0 1]
:  [2 3 2 1 1 1 2 1 1]]

** TF-IDFを使って単語の関連性を評価する
*TF-IDF* は、 *TF* （単語の出現頻度）と *IDF* （逆文章頻度）の積として定義できる。

#+begin_src python :session :results output
  from sklearn.feature_extraction.text import TfidfTransformer
  tfidf = TfidfTransformer(use_idf = True, norm = "l2", smooth_idf = True)
  np.set_printoptions(precision = 2)
  print(tfidf.fit_transform(count.fit_transform(docs)).toarray())

#+end_src

#+RESULTS:
: [[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]
:  [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]
:  [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]

** テキストデータのクレンジング
テキストデータをクレンジングすることが最初の重要な手順となる
#+begin_src python :session :results value
  df.loc[0, "review"][-50:]
#+end_src

#+RESULTS:
: is seven.<br /><br />Title (Brazil): Not Available

#+begin_src python :session :results value
  import re
  def preprocessor(text):
      text = re.sub("<[^>]*>", "", text)
      emoticons = re.findall("(?::|;|=)(?:-)?(?:\)|\(|D|P)", text)
      return (re.sub("[\W]+", " ", text.lower()) +
              " ".join(emoticons).replace("-", ""))
#+end_src

#+RESULTS:

#+begin_src python :session :results value
  preprocessor(df.loc[0, "review"][-50:])
#+end_src

#+RESULTS:
: is seven title brazil not available

#+begin_src python :session :results value
  preprocessor("</a>This :) is :( a test :-)!")
#+end_src

#+RESULTS:
: this is a test :) :( :)

#+begin_src python :session :results value
  df["review"] = df["review"].apply(preprocessor)
#+end_src

#+RESULTS:

** 文章をトークン化

#+begin_src python :session :results value
  def tokenizer(text):
      return text.split()

  tokenizer("runners like running and thus they run")

#+end_src

#+RESULTS:
| runners | like | running | and | thus | they | run |

Porterステミングアルゴリズム
#+begin_src python :session :results value
  from nltk.stem.porter import PorterStemmer
  porter = PorterStemmer()
  def tokenizer_porter(text):
      return [porter.stem(word) for word in text.split()]

  tokenizer_porter("runners like running and thus they run")
#+end_src

#+RESULTS:
| runner | like | run | and | thu | they | run |

*ストップワードの除去*
ストップワードとは、あらゆる種類のテキストに見られるごくありふれた単語のこと
is, and, has, likeなど

#+begin_src python :session :results value
  import nltk
  nltk.download("stopwords")
#+end_src

#+RESULTS:
: True

#+begin_src python :session :results value
  from nltk.corpus import stopwords
  stop = stopwords.words("english")
  [w for w in tokenizer_porter("a runner likes running and runs a lot")[-10:]
   if w not in stop]
#+end_src

#+RESULTS:
| runner | like | run | run | lot |

** 文章を分類するロジスティック回帰モデルの訓練
25,000個の訓練用の文章と25,000個のテスト用の文章に分割する
#+begin_src python :session :results value
  X_train = df.loc[:25000, "review"].values
  y_train = df.loc[:25000, "sentiment"].values
  X_test = df.loc[25000:, "review"].values
  y_test = df.loc[25000:, "sentiment"].values
#+end_src

#+RESULTS:

GridSearchCVオブジェクトを使ってロジスティック回帰モデルの最適なパラメータ集合を求める。
5分割交差検証
#+begin_src python :session :results value
  from sklearn.model_selection import GridSearchCV
  from sklearn.pipeline import Pipeline
  from sklearn.linear_model import LogisticRegression
  from sklearn.feature_extraction.text import TfidfVectorizer

  tfidf = TfidfVectorizer(strip_accents = None,
                         lowercase = False,
                         preprocessor = None)
  param_grid = [{"vect__ngram_range": [(1, 1)],
                 "vect__stop_words": [stop, None],
                 "vect__tokenizer": [tokenizer, tokenizer_porter],
                 "clf__penalty": ["l1", "l2"],
                 "clf__C": [1.0, 10.0, 100.0]},
                {"vect__ngram_range": [(1, 1)],
                 "vect__stop_words": [stop, None],
                 "vect__tokenizer": [tokenizer, tokenizer_porter],
                 "vect__use_idf": [False],
                 "vect__norm": [None],
                 "clf__penalty": ["l1", "l2"],
                 "clf__C": [1.0, 10.0, 100.0]}]
  lr_tfidf = Pipeline([("vect", tfidf),
                       ("clf", LogisticRegression(random_state = 0,
                                                  solver = "liblinear"))])
  gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,
                             scoring = "accuracy",
                             cv = 5, verbose = 2,
                             n_jobs = -1)
  gs_lr_tfidf.fit(X_train, y_train)
#+end_src

#+RESULTS:
#+begin_example
GridSearchCV(cv=5,
             estimator=Pipeline(steps=[('vect',
                                        TfidfVectorizer(lowercase=False)),
                                       ('clf',
                                        LogisticRegression(random_state=0,
                                                           solver='liblinear'))]),
             n_jobs=-1,
             param_grid=[{'clf__C': [1.0, 10.0, 100.0],
                          'clf__penalty': ['l1', 'l2'],
                          'vect__ngram_range': [(1, 1)],
                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',
                                                'our', 'ours', 'ourselves',
                                                'you', "you're", "you've...
                                                'our', 'ours', 'ourselves',
                                                'you', "you're", "you've",
                                                "you'll", "you'd", 'your',
                                                'yours', 'yourself',
                                                'yourselves', 'he', 'him',
                                                'his', 'himself', 'she',
                                                "she's", 'her', 'hers',
                                                'herself', 'it', "it's", 'its',
                                                'itself', ...],
                                               None],
                          'vect__tokenizer': [<function tokenizer at 0x7f4e6718b1a0>,
                                              <function tokenizer_porter at 0x7f4e8209e660>],
                          'vect__use_idf': [False]}],
             scoring='accuracy', verbose=2)
#+end_example

#+begin_src python :session :results output
  print("Best parameter set: %s" % gs_lr_tfidf.best_params_)
#+end_src

#+RESULTS:
: Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7f4e6718b1a0>}

グリッドサーチによって得られた最良のモデルを使って、訓練データ・セットでの5分割交差検証の正解率の平均と、テストデータセットの正解率を出力する
#+begin_src python :session :results output
  print("CV Accuracy: %.3f" % gs_lr_tfidf.best_score_)
  clf = gs_lr_tfidf.best_estimator_
  print("Test Accuracy: %.3f" % clf.score(X_test, y_test))

#+end_src

#+RESULTS:
: CV Accuracy: 0.897
: Test Accuracy: 0.899

映画レビューが肯定的か否定的かをほぼ90%の正解率で予測できる

* さらに大規模なデータの処理：オンラインアルゴリズムとアウトオブコア学習
データセットの小さなバッチを使って分類器を逐次的に適合させる

`tokenizer`関数
#+begin_src python :session :results output
  import numpy as np
  import re
  from nltk.corpus import stopwords
  stop = stopwords.words("english")
  def tokenizer(text):
      text = re.sub("<[^>]*>", "", text)
      emoticons = re.findall("(?::|;|=)(?:-)?(?:\)|\(|D|P)", text)
      text = re.sub("[\W]+", " ", text.lower()) + " ".join(emoticons).replace("-", "")
      tokenized = [w for w in text.split() if w not in stop]
      return tokenized
#+end_src

#+RESULTS:
: /tmp/babel-Oj9kZ8/python-0h4XvQ:7: SyntaxWarning: invalid escape sequence '\)'
:   emoticons = re.findall("(?::|;|=)(?:-)?(?:\)|\(|D|P)", text)
: /tmp/babel-Oj9kZ8/python-0h4XvQ:8: SyntaxWarning: invalid escape sequence '\W'
:   text = re.sub("[\W]+", " ", text.lower()) + " ".join(emoticons).replace("-", "")

ジェネレータ関数`stream_docs`
#+begin_src python :session :results output
  def stream_docs(path):
      with open(path, "r", encoding="utf-8") as csv:
          next(csv)               # ヘッダーを読み飛ばす
          for line in csv:
              text, label = line[:-3], int(line[-2])
              yield text, label
#+end_src

#+RESULTS:

#+begin_src python :session :results value
  next(stream_docs(path = "movie_data.csv"))
#+end_src

#+RESULTS:
| "In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70's, they discover the criminal and a net of power and money to cover the murder.<br /><br />""Murder in Greenwich"" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available" | 1 |

`get_minibatch`関数
#+begin_src python :session :results output
  def get_minibatch(doc_stream, size):
      docs, y = [], []
      try:
          for _ in range(size):
              text, label = next(doc_stream)
              docs.append(text)
              y.append(label)
      except StopIteration:
          return None, None
      return docs, y
#+end_src

#+RESULTS:

アウトオブコア学習では、語彙が完全にメモリに読み込まれていることが要求される。
#+begin_src python :session :results output
  def get_minibatch(doc_stream, size):
      docs, y = [], []
      try:
          for _ in range(size):
              text, label = next(doc_stream)
              docs.append(text)
              y.append(label)
      except StopIteration:
          return None, None
      return docs, y
#+end_src

#+RESULTS:

#+begin_src python :session :results output
  from sklearn.feature_extraction.text import HashingVectorizer
  from sklearn.linear_model import SGDClassifier
  vect = HashingVectorizer(decode_error = "ignore",
                           n_features = 2**21,
                           preprocessor = None,
                           tokenizer = tokenizer)
  clf = SGDClassifier(loss = "log_loss", random_state = 1)
  doc_stream = stream_docs(path = "movie_data.csv")
#+end_src

#+RESULTS:

#+begin_src python :session
  import pyprind
  pbar = pyprind.ProgBar(45)
  classes = np.array([0, 1])
  for _ in range(45):
      X_train, y_train = get_minibatch(doc_stream, size = 1000)
      if not X_train:
          break
      X_train = vect.transform(X_train)
      clf.partial_fit(X_train, y_train, classes = classes)
      pbar.update()
#+end_src

#+RESULTS:

#+begin_src python :session :results output
  X_test, y_test = get_minibatch(doc_stream, size = 5000)
  X_test = vect.transform(X_test)
  print("Accuracy: %.3f" % clf.score(X_test, y_test))
#+end_src

#+RESULTS:
: Accuracy: 0.868

#+begin_src python :session :results output
  clf = clf.partial_fit(X_test, y_test)
#+end_src

#+RESULTS:

* 潜在ディリクレ配分によるトピックモデルの構築

** 潜在ディリクレ配分を使ってテキスト文章を分解する
潜在ディリクレ配分(LDA)
トピックの個数は、明示的に指定しなければならないLDAのハイパーパラメータ

** scikit-learnの潜在ディリクレ配分

#+begin_src python :session :results output
  import pandas as pd
  df = pd.read_csv("movie_data.csv", encoding = "utf-8")
#+end_src

#+RESULTS:

#+begin_src python :session :results output
  from sklearn.feature_extraction.text import CountVectorizer
  count = CountVectorizer(stop_words = "english",
                          max_df = 0.1,
                          max_features = 5000)
  X = count.fit_transform(df["review"].values)
#+end_src

#+RESULTS:

#+begin_src python :session :results output
  from sklearn.decomposition import LatentDirichletAllocation
  lda = LatentDirichletAllocation(
      n_components = 10,
      random_state = 123,
      learning_method = "batch")
  X_topics = lda.fit_transform(X)
#+end_src

#+RESULTS:

#+begin_src python :session :results value
  lda.components_.shape
#+end_src

#+RESULTS:
| 10 | 5000 |

10種類のトピック（昇順）ごとに、単語の重要度（5,000個）を含んだ行列が含まれる
10種類のトピックごとに最も重要な5つの単語を出力する

#+begin_src python :session :results output
  n_top_words = 5
  feature_names = count.get_feature_names_out()
  for topic_idx, topic in enumerate(lda.components_):
      print("Topic %d:" % (topic_idx + 1))
      print(" ".join([feature_names[i] for i in topic.argsort() \
                      [:-n_top_words - 1:-1]]))

#+end_src

#+RESULTS:
#+begin_example
Topic 1:
horror effects budget special gore
Topic 2:
guy worst money minutes stupid
Topic 3:
version action japanese english match
Topic 4:
book audience human feel documentary
Topic 5:
series tv episode shows episodes
Topic 6:
family woman father mother girl
Topic 7:
music musical role performance song
Topic 8:
war police men murder action
Topic 9:
script comedy role actor performance
Topic 10:
comedy original action watched fan
#+end_example

#+begin_src python :session :results output
  horror = X_topics[:, 0].argsort()[::-1]
  for iter_idx, movie_idx in enumerate(horror[:3]):
      print("\nHorror movie #%d:" % (iter_idx + 1))
      print(df["review"][movie_idx][:300], "...")
#+end_src

#+RESULTS:
:
: Horror movie #1:
: Over Christmas break, a group of college friends stay behind to help prepare the dorms to be torn down and replaced by apartment buildings. To make the work a bit more difficult, a murderous, Chucks-wearing psycho is wandering the halls of the dorm, preying on the group in various violent ways.<br / ...
:
: Horror movie #2:
: A group of friends discover gold deep inside an old mine. But by taking the gold and thinking they've hit it big, they awaken a long dead miner who's Hell Bent on protecting his treasure. "Miner's Massacre" is a chintzy b-horror movie in the extreme. You've got all your familiar clichés, your group  ...
:
: Horror movie #3:
: God Bless 80's slasher films. This is a fun, fun movie. This is what slasher films are all about. Now I'm not saying horror movies, just slasher films. It goes like this: A high school nerd is picked on by all these stupid jocks and cheerleaders, and then one of their pranks goes horribly wrong. Dis ...
