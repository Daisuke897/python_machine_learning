*回帰分析 ―連続値をとる目的変数の予測*

#+begin_src emacs-lisp
  (pyvenv-activate "~/project/python_machine_learning")
#+end_src

#+RESULTS:
* 線形回帰
* Housingデータセットの探索
** Housingデータセットをデータフレームに読み込む
目的変数として住宅価格の中央値（MEDV）を使う
#+begin_src python :session :results value
  import pandas as pd

  df = pd.read_csv("https://raw.githubusercontent.com/rasbt/"
                   "python-machine-learning-book-3rd-edition/"
                   "master/ch10/housing.data.txt",
                   header=None,
                   sep="\s+")

  df.columns = ["CRIM", "ZN", "INDUS", "CHAS",
                "NOX", "RM", "AGE", "DIS", "RAD",
                "TAX", "PTRATIO", "B", "LSTAT", "MEDV"]

  df.head(5)
#+end_src

#+RESULTS:
:       CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  PTRATIO       B  LSTAT  MEDV
: 0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0     15.3  396.90   4.98  24.0
: 1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0     17.8  396.90   9.14  21.6
: 2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0     17.8  392.83   4.03  34.7
: 3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0     18.7  394.63   2.94  33.4
: 4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0     18.7  396.90   5.33  36.2
** データセットの重要な特性を可視化する
*探索的データ解析* 機械学習モデルを訓練する前の最初の重要なステップとして推奨
*散布図行列* を作成する

#+begin_src python :session :results file link
  import matplotlib.pyplot as plt
  from mlxtend.plotting import scatterplotmatrix
  cols = ["LSTAT", "INDUS", "NOX", "RM", "MEDV"]
  plt.close("all")
  scatterplotmatrix(df[cols].values, figsize=(10, 8), names = cols, alpha = 0.5)
  plt.tight_layout()

  fname = "images/10_03.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/10_03.png]]
** 相関行列を使って関係を調べる

#+begin_src python :session :results file link
  from mlxtend.plotting import heatmap
  import numpy as np
  plt.close("all")
  cm = np.corrcoef(df[cols].values.T) # ピアソンの積率相関係数を計算
  hm = heatmap(cm, row_names = cols, column_names = cols)
  fname = "images/10_04.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/10_04.png]]

* 最小二乗線形回帰モデルの実装

** 勾配降下法を使って回帰パラメータの回帰を解く

#+begin_src python :session
  # 基本的な線形回帰モデル
  class LinearRegressionGD(object):

      # 初期化を実行する__init__
      def __init__(self, eta=0.001, n_iter = 20):
          self.eta = eta          # 学習率
          self.n_iter = n_iter    # 訓練回数

      # 訓練を実行するfit
      def fit(self, X, y):
          self.w_ = np.zeros(1 + X.shape[1]) # 重みを初期化
          self.cost_ = []                    # コスト関数の値を初期化
          for i in range(self.n_iter):
              output = self.net_input(X) # 活性化関数の出力を計算
              errors = (y - output)      # 誤差を計算
              self.w_[1:] += self.eta * X.T.dot(errors) # 重みw_{1}以降を更新
              self.w_[0] += self.eta * errors.sum()     # 重みw_{0}を更新
              cost = (errors ** 2).sum() / 2.0          # コスト関数を計算
              self.cost_.append(cost)                   # コスト関数の値を格納
          return self

      # 総入力を計算するnet_input
      def net_input(self, X):
          return np.dot(X, self.w_[1:]) + self.w_[0]

      # 予測値を計算するpredict
      def predict(self, X):
          return self.net_input(X)
#+end_src

#+RESULTS:

説明変数としてRM（1戸あたりの平均部屋数）を使う
#+begin_src python :session :results value
  X = df[["RM"]].values
  y = df["MEDV"].values
  from sklearn.preprocessing import StandardScaler
  sc_x = StandardScaler()
  sc_y = StandardScaler()
  X_std = sc_x.fit_transform(X)
  y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()
  lr = LinearRegressionGD()
  lr.fit(X_std, y_std)
#+end_src

#+RESULTS:
: <__main__.LinearRegressionGD object at 0x7ff62269daf0>

#+begin_src python :session :results file link
  # エポック数とコストの関係を表す折れ線グラフのプロット
  plt.close("all")
  plt.plot(range(1, lr.n_iter + 1), lr.cost_)
  plt.ylabel("SSE")
  plt.xlabel("Epoch")

  fname = "images/10_05.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/10_05.png]]

線形回帰の直線が訓練データにどの程度適合しているかを可視化する

#+begin_src python :session
  def lin_regplot(X, y, model):
      plt.scatter(X, y, c = "steelblue", edgecolor = "white", s = 70)
      plt.plot(X, model.predict(X), color = "black", lw = 2)
      return None
#+end_src

#+RESULTS:

#+begin_src python :session :results file link
  plt.close("all")
  lin_regplot(X_std, y_std, lr)
  plt.xlabel("Average number of rooms [RM] (standardized)")
  plt.ylabel("Price in $1000s [MEDV] (standardized)")

  fname = "images/10_06.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/10_06.png]]

#+begin_src python :session :results output
  num_rooms_std = sc_x.transform(np.array([[5.0]]))
  price_std = lr.predict(num_rooms_std)
  print("Price in $1000s: %.3f" % sc_y.inverse_transform(price_std[:, np.newaxis]).flatten())
#+end_src

#+RESULTS:
: /tmp/babel-SlMf7Y/python-x9uSmP:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
:   print("Price in $1000s: %.3f" % sc_y.inverse_transform(price_std[:, np.newaxis]).flatten())
: Price in $1000s: 10.840

#+begin_src python :session :results output
  print("Slope: %.3f" % lr.w_[1])
  print("Intercept: %.3f" % lr.w_[0])
#+end_src

#+RESULTS:
: Slope: 0.695
: Intercept: -0.000

** scikit-learnを使って回帰モデルの係数を推定する
#+begin_src python :session :results output
  from sklearn.linear_model import LinearRegression
  slr = LinearRegression()
  slr.fit(X, y)
  y_pred = slr.predict(X)
  print("Slope: %.3f" % slr.coef_[0])
  print("Intercept: %.3f" % slr.intercept_)
#+end_src

#+RESULTS:
: Slope: 9.102
: Intercept: -34.671

#+begin_src python :session :results file link
  plt.close("all")
  lin_regplot(X, y, slr)
  plt.xlabel("Average number of rooms [RM]")
  plt.ylabel("Price in $1000s [MEDV]")

  fname = "images/10_07.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/10_07.png]]

* RANSACを使ったロバスト回帰モデルの学習
*RANSAC* アルゴリズム
回帰モデルにデータのサブセットいわゆる正常値を学習させる
RANSACアルゴリズムと線形モデルを組み合わせる
#+begin_src python :session :results value
  from sklearn.linear_model import RANSACRegressor
  # RANSACモデルをインスタンス化
  ransac = RANSACRegressor(LinearRegression(),
                           max_trials = 100,
                           min_samples = 50,
                           loss = "absolute_error",
                           residual_threshold = 5.0,
                           random_state = 0)
  ransac.fit(X, y)
#+end_src

#+RESULTS:
: RANSACRegressor(estimator=LinearRegression(), min_samples=50, random_state=0,
:                 residual_threshold=5.0)

#+begin_src python :session :results file link
  inlier_mask = ransac.inlier_mask_ # 正常値を表す真偽値を取得
  outlier_mask = np.logical_not(inlier_mask) # 外れ値を表す真偽値を取得
  line_X = np.arange(3, 10, 1)               # 3から9までの整数値を作成
  line_y_ransac = ransac.predict(line_X[:, np.newaxis]) # 予測値を計算
  plt.close("all")
  # 正常値をプロット
  plt.scatter(X[inlier_mask], y[inlier_mask],
              c = "steelblue", edgecolor = "white", marker = "o", label = "Inliers")
  # 外れ値をプロット
  plt.scatter(X[outlier_mask], y[outlier_mask],
              c = "limegreen", edgecolor = "white", marker = "s", label = "Outliers")
  # 予測値をプロットする
  plt.plot(line_X, line_y_ransac, color = "black", lw = 2)
  plt.xlabel("Average number of rooms [RM]")
  plt.ylabel("Price in $1000s [MEDV]")
  plt.legend(loc = "upper left")

  fname = "images/10_08.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/10_08.png]]

モデルの切片と傾き
#+begin_src python :session :results output
  print("Slope: %.3f" % ransac.estimator_.coef_[0])
  print("Intercept: %.3f" % ransac.estimator_.intercept_)
#+end_src

#+RESULTS:
: Slope: 10.735
: Intercept: -44.089

* 線形回帰モデルの性能評価
#+begin_src python :session :results output
  from sklearn.model_selection import train_test_split
  X = df.iloc[:, :-1].values
  y = df["MEDV"].values
  X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                      test_size = 0.3, random_state = 0)
  slr = LinearRegression()
  slr.fit(X_train, y_train)
  y_train_pred = slr.predict(X_train)
  y_test_pred = slr.predict(X_test)
#+end_src

#+RESULTS:

*残差プロット* を作成する
#+begin_src python :session :results file link
  plt.close("all")
  plt.scatter(y_train_pred, y_train_pred - y_train,
              c = "steelblue", marker = "o", edgecolor = "white", label = "Training data")
  plt.scatter(y_test_pred, y_test_pred - y_test,
              c = "limegreen", marker = "s", edgecolor = "white", label = "Test data")
  plt.xlabel("Predicted Value")
  plt.ylabel("Residuals")
  plt.legend(loc = "upper left")
  plt.hlines(y = 0, xmin = -10, xmax = 50, color = "black", lw = 2)
  plt.xlim([-10, 50])
  plt.tight_layout()

  fname = "images/10_09.png"

  plt.savefig(fname)

  fname
#+end_src

#+RESULTS:
[[file:images/10_09.png]]

*平均二乗誤差(MSE)*
#+begin_src python :session :results output
  from sklearn.metrics import mean_squared_error
  # 平均二条誤差を出力
  print("MSE train: %.3f, test: %.3f" % (mean_squared_error(y_train, y_train_pred),
                                         mean_squared_error(y_test, y_test_pred)))
#+end_src

#+RESULTS:
: MSE train: 19.958, test: 27.196

過学習？

*決定係数*
#+begin_src python :session :results output
  # R^2（決定係数）のスコアを出力
  from sklearn.metrics import r2_score
  print("R^2 train: %.3f, test: %.3f" % (r2_score(y_train, y_train_pred),
                                         r2_score(y_test, y_test_pred)))
#+end_src

#+RESULTS:
: R^2 train: 0.765, test: 0.673

* 回帰に正則化手法を使う
正則化された線形回帰の最も一般的なアプローチ
- リッジ回帰
  L2ペナルティ
- LASSO
  L1ペナルティ
- Elastic Net法
  L1ペナルティとL2ペナルティ

#+begin_src python :session :results output
  from sklearn.linear_model import Ridge
  ridge = Ridge(alpha = 1.0)      # L2ペナルティ項の影響度合いを表す値を引数に指定
#+end_src

#+RESULTS:

#+begin_src python :session :results output
  from sklearn.linear_model import Lasso
  lasso = Lasso(alpha = 1.0)
#+end_src

#+RESULTS:

#+begin_src python :session :results output
  from sklearn.linear_model import ElasticNet
  elanet = ElasticNet(alpha = 1.0, l1_ratio = 0.5)
#+end_src

#+RESULTS:

* 多項式回帰：線形回帰モデルから曲線を見出す

** scikit-learnを使って多項式の項を追加する
1. 多項式の2次の項を追加する
   #+begin_src python :session :results output
     from sklearn.preprocessing import PolynomialFeatures
     X = np.array([258.0, 270.0, 294.0,
                   320.0, 342.0, 368.0,
                   396.0, 446.0, 480.0, 586.0])\
                  [:, np.newaxis]

     y = np.array([236.4, 234.4, 252.8,
                   298.6, 314.2, 342.2,
                   360.8, 368.0, 391.2,
                   390.8])
     # 線形回帰（最小二乗法）モデルのクラスをインスタンス化
     lr = LinearRegression()
     pr = LinearRegression()
     # 2次の多項式特徴量のクラスをインスタンス化
     quadratic = PolynomialFeatures(degree = 2)
     X_quad = quadratic.fit_transform(X)
   #+end_src

   #+RESULTS:

2. 比較を可能にするために、単回帰モデルを学習させる
   #+begin_src python :session :results output
      lr.fit(X, y)
      # np.newaxisで列ベクトルにする
      X_fit = np.arange(250, 600, 10)[:, np.newaxis]
      # 予測値を計算
      y_lin_fit = lr.predict(X_fit)
   #+end_src

   #+RESULTS:

3. 多項式回帰のために、変換された特徴量で重回帰モデルを学習させる
   #+begin_src python :session :results output
      pr.fit(X_quad, y)
      # 2次式でyの値を計算
      y_quad_fit = pr.predict(quadratic.fit_transform(X_fit))
   #+end_src

   #+RESULTS:

4. 結果をプロットする
   #+begin_src python :session :results file link
     # 散布図、線型回帰モデル、多項式回帰モデルの結果をプロット

     plt.close("all")

     plt.scatter(X, y, label = "Training points")
     plt.plot(X_fit, y_lin_fit, label = "Linear fit", linestyle = "--")
     plt.plot(X_fit, y_quad_fit, label = "Quadratic fit")
     plt.xlabel("Explanatory variable")
     plt.ylabel("Predicted or known target values")
     plt.legend(loc = "upper left")
     plt.tight_layout()

     fname = "images/10_11.png"

     plt.savefig(fname)

     fname
   #+end_src

   #+RESULTS:
   [[file:images/10_11.png]]

5. 評価手法として平均二条誤差とR2を計算する
   #+begin_src python :session :results output
     y_lin_pred = lr.predict(X)
     y_quad_pred = pr.predict(X_quad)
     print("Training MSE linear: %.3f, quadratic: %.3f" %
           (mean_squared_error(y, y_lin_pred), mean_squared_error(y, y_quad_pred)))
     print("Training R^2 linear: %.3f, quadratic: %.3f" %
           (r2_score(y, y_lin_pred), r2_score(y, y_quad_pred)))
   #+end_src

   #+RESULTS:
   : Training MSE linear: 569.780, quadratic: 61.330
   : Training R^2 linear: 0.832, quadratic: 0.982

**  Housingデータセットで非線形関係をモデル化する
#+begin_src python :session :results file link
  X = df[["LSTAT"]].values
  y = df["MEDV"].values
  regr = LinearRegression()

  quadratic = PolynomialFeatures(degree=2)
  cubic = PolynomialFeatures(degree=3)
  X_quad = quadratic.fit_transform(X)
  X_cubic = cubic.fit_transform(X)

  X_fit = np.arange(X.min(), X.max(), 1)[:,  np.newaxis]
  regr = regr.fit(X, y)
  y_lin_fit = regr.predict(X_fit)
  linear_r2 = r2_score(y, regr.predict(X))

  regr = regr.fit(X_quad, y)
  y_quad_fit = regr.predict(quadratic.fit_transform(X_fit))
  quadratic_r2 = r2_score(y, regr.predict(X_quad))

  regr = regr.fit(X_cubic, y)
  y_cubic_fit = regr.predict(cubic.fit_transform(X_fit))
  cubic_r2 = r2_score(y, regr.predict(X_cubic))

  plt.close("all")

  plt.scatter(X, y, label = "Training points", color = "lightgray")
  plt.plot(X_fit, y_lin_fit, label = "Linear (d=1), $R^2 = %.2f$" %
           linear_r2, color = "blue", lw = 2, linestyle = ":")
  plt.plot(X_fit, y_quad_fit, label = "Quadratic (d=2), $R^2 = %.2f$" %
           quadratic_r2, color = "red", lw = 2, linestyle = "-")
  plt.plot(X_fit, y_cubic_fit, label = "Cubic (d=3), $R^2 = %.2f$" %
           cubic_r2, color = "green", lw = 2, linestyle = "--")
  plt.xlabel("% lower status of population [LSTAT]")
  plt.ylabel("Price in $1000s [MEDV]")
  plt.legend(loc = "upper right")

  fname = "images/10_12.png"

  plt.savefig(fname)

  fname
#+end_src

#+RESULTS:
[[file:images/10_12.png]]

#+begin_src python :session :results file link
  # 特徴量を変換
  X_log = np.log(X)
  y_sqrt = np.sqrt(y)

  # 特徴量の学習、予測、決定係数の計算
  X_fit = np.arange(X_log.min() - 1, X_log.max() + 1, 1)[:, np.newaxis]
  regr = regr.fit(X_log, y_sqrt)
  y_lin_fit = regr.predict(X_fit)
  linear_r2 = r2_score(y_sqrt, regr.predict(X_log))

  # 射影したデータを使った学習結果をプロット
  plt.close("all")
  plt.scatter(X_log, y_sqrt, label = "Training points", color = "lightgray")
  plt.plot(X_fit, y_lin_fit, label = "Linear (d = 1), $R^2 = %.2f" % linear_r2,
           color = "blue", lw = 2)
  plt.xlabel("log(% lower status of the population [LSTAT])")
  plt.ylabel("$\sqrt(Price \; in \; \$1000s [MEDV])")
  plt.legend(loc = "lower left")
  plt.tight_layout()

  fname = "images/10_13.png"

  plt.savefig(fname)

  fname

#+end_src

#+RESULTS:
[[file:images/10_13.png]]

** ランダムフォレストを使って非線形関係に対処する

#+begin_src python :session :results file link
  plt.close("all")

  fname = "images/10_14.png"

  plt.savefig(fname)

  fname

#+end_src
