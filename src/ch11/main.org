*クラスタ分析 ―ラベルなしのデータ分析*

*教師なし学習法* に属するクラスタ分析について

#+begin_src emacs-lisp
  (pyvenv-activate "~/project/python_machine_learning")
#+end_src

#+RESULTS:

* k-means法を使った類似度によるオブジェクトのグループ化

** scikit-learnを使ったk-meansクラスタリング
k-means法の問題点の1つは、クラスタの個数kを指定しなければならないこと

#+begin_src python :session :results file link
  from sklearn.datasets import make_blobs

  X, y = make_blobs(n_samples = 150, # データ点の個数
                    n_features = 2,  # 特徴量の個数
                    centers = 3,     # クラスタの個数
                    cluster_std = 0.5, # クラスタ内の標準偏差
                    shuffle = True,    # データ点のシャッフル
                    random_state = 0)  # 乱数生成器の状態を指定

  import matplotlib.pyplot as plt
  plt.scatter(X[:, 0], X[:, 1], c = "white", marker = "o", edgecolor = "black", s = 50)
  plt.grid()
  plt.tight_layout()

  fname = "images/11_01.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_01.png]]

#+begin_src python :session :results output
  from sklearn.cluster import KMeans

  km = KMeans(n_clusters = 3,     # クラスタの個数
              init = "random",    # セントロイドの初期値をランダムに選択
              n_init = 10,
              max_iter = 300,     # k-meansアルゴリズム内部の最大イテレーション回数
              tol = 1e-04,        # 収束と判定するための相対的な許容誤差
              random_state = 0)   # セントロイドの初期化に用いる乱数生成器の状態
  y_km = km.fit_predict(X)        # クラスタ中心の計算と各データ点のインデックスの予測
#+end_src

#+RESULTS:

#+begin_src python :session :results file link
  plt.close("all")
  plt.scatter(X[y_km == 0, 0],
              X[y_km == 0, 1],
              s = 50,
              c = "lightgreen",
              edgecolor = "black",
              marker = "s",
              label = "Cluster 1")
  plt.scatter(X[y_km == 1, 0],
              X[y_km == 1, 1],
              s = 50,
              c = "orange",
              edgecolor = "black",
              marker = "o",
              label = "Cluster 2")
  plt.scatter(X[y_km == 2, 0],
              X[y_km == 2, 1],
              s = 50,
              c = "lightblue",
              edgecolor = "black",
              marker = "v",
              label = "Cluster 3")
  plt.scatter(km.cluster_centers_[:,0],
              km.cluster_centers_[:,1],
              s = 250,
              marker = "*",
              c = "red",
              edgecolor = "black",
              label = "Centroids")
  plt.legend(scatterpoints = 1)
  plt.grid()
  plt.tight_layout()

  fname = "images/11_02.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_02.png]]

** k-means++法を使ってセントロイドの初期値をよりスマートに設定する

** ハードクラスタリングとソフトクラスタリング
- *ハードクラスタリング* データセットのデータ点がそれぞれちょうど1つのクラスタに割り当てられるアルゴリズム
- *ソフトクラスタリング* データ点を1つ以上のクラスタに割り当てるアルゴリズム

** エルボー法を使ってクラスタの最適な個数を求める
#+begin_src python :session :results output
  print("Distortion: %.2f" % km.inertia_)
#+end_src

#+RESULTS:
: Distortion: 72.48

#+begin_src python :session :results file link
  distortions = []

  for i in range(1, 11):
      km = KMeans(
          n_clusters = i,
          init = 'k-means++',
          n_init = 10,
          max_iter = 300,
          random_state = 0)
      km.fit(X)
      distortions.append(km.inertia_)

  plt.close("all")
  plt.plot(range(1, 11), distortions, marker = "o")
  plt.xlabel("Number of clusters")
  plt.ylabel("Distortion")
  plt.tight_layout()
  fname = "images/11_03.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_03.png]]

k=3が良さそう

** シルエット図を使ってクラスタリングの性能を数値化する
*シルエット分析* はk-means法以外のクラスタリングアルゴリズムにも適用できる。
クラスタ内のデータ点がどの程度密にグループ化されているか

#+begin_src python :session :results file link
  km = KMeans(n_clusters = 3,
              init = 'k-means++',
              n_init = 10,
              max_iter = 300,
              tol = 1e-04,
              random_state = 0)
  y_km = km.fit_predict(X)

  import numpy as np
  from matplotlib import cm
  from sklearn.metrics import silhouette_samples
  cluster_labels = np.unique(y_km) # y_kmの要素の中で重複をなくす
  n_clusters = cluster_labels.shape[0] # 配列の長さを返す
  # シルエット係数を計算
  silhouette_vals = silhouette_samples(X, y_km, metric = "euclidean")
  y_ax_lower, y_ax_upper = 0, 0
  yticks = []

  plt.close("all")
  for i, c in enumerate(cluster_labels):
      c_silhouette_vals = silhouette_vals[y_km == c]
      c_silhouette_vals.sort()
      y_ax_upper += len(c_silhouette_vals)
      color = cm.jet(float(i) / n_clusters) # 色の値をセット
      plt.barh(range(y_ax_lower, y_ax_upper), # 水平の棒を描画
               c_silhouette_vals,             # 棒の幅
               height = 1.0,                  # 棒の高さ
               edgecolor = 'none',            # 棒の端の色
               color = color)                 # 棒の色
      yticks.append((y_ax_lower + y_ax_upper) / 2.) # クラスタレベルの表示位置を追加
      y_ax_lower += len(c_silhouette_vals)          # 底辺の棒の幅を追加

  silhouette_avg = np.mean(silhouette_vals) # シルエット係数の平均値
  plt.axvline(silhouette_avg, color = "red", linestyle = "--") # 係数の平均値に破線を引く
  plt.yticks(yticks, cluster_labels + 1)                       # クラスラベルを表示
  plt.ylabel("Cluster")
  plt.xlabel("Silhouette coefficient")
  plt.tight_layout()

  fname = "images/11_04.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_04.png]]

シルエット係数の値が1であれば「よいクラスタリング」を示すことになる。
このシルエット図は1から程遠い。

比較的悪いクラスタリングならどうなる？
#+begin_src python :session :results file link
  km = KMeans(n_clusters = 2,
              init = "k-means++",
              n_init = 10,
              max_iter = 300,
              tol = 1e-04,
              random_state = 0)
  y_km = km.fit_predict(X)
  plt.close("all")
  plt.scatter(X[y_km == 0, 0],
              X[y_km == 0, 1],
              s = 50,
              c = "lightgreen",
              edgecolor = "black",
              marker = "s",
              label = "Cluster 1")
  plt.scatter(X[y_km == 1, 0],
              X[y_km == 1, 1],
              s = 50,
              c = "orange",
              edgecolor = "black",
              marker = "o",
              label = "Cluster 2")
  plt.scatter(km.cluster_centers_[:, 0],
              km.cluster_centers_[:, 1],
              s = 250,
              marker = "*",
              c = "red",
              label = "Centroids")
  plt.legend()
  plt.grid()
  plt.tight_layout()

  fname = "images/11_05.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_05.png]]

結果を評価するシルエット図を作成する
#+begin_src python :session :results file link
  cluster_labels = np.unique(y_km)
  n_clusters = cluster_labels.shape[0]
  silhouette_vals = silhouette_samples(X, y_km, metric = "euclidean")
  y_ax_lower, y_ax_upper = 0, 0
  yticks = []

  plt.close("all")
  for i, c in enumerate(cluster_labels):
      c_silhouette_vals = silhouette_vals[y_km == c]
      c_silhouette_vals.sort()
      y_ax_upper += len(c_silhouette_vals)
      color = cm.jet(float(i) / n_clusters)
      plt.barh(range(y_ax_lower, y_ax_upper),
               c_silhouette_vals,
               height = 1.0,
               edgecolor = "none",
               color = color)
      yticks.append((y_ax_lower + y_ax_upper) / 2.)
      y_ax_lower += len(c_silhouette_vals)

  silhouette_avg = np.mean(silhouette_vals)
  plt.axvline(silhouette_avg, color = "red", linestyle = "--")
  plt.yticks(yticks, cluster_labels + 1)
  plt.ylabel("Cluster")
  plt.xlabel("Silhouette coefficient")
  plt.tight_layout()

  fname = "images/11_06.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_06.png]]

* クラスタを階層木として構成する

** ボトムアップ方式でのクラスタのグループ化

完全連結法に基づく凝集型階層的クラスタリングは反復的な手続き

ランダムなデータ点を生成する
#+begin_src python :session :results value
  import pandas as pd
  import numpy as np
  np.random.seed(123)
  variables = ["X", "Y", "Z"]
  labels = ["ID_0", "ID_1", "ID_2", "ID_3", "ID_4"]
  X = np.random.random_sample([5, 3]) * 10
  df = pd.DataFrame(X, columns = variables, index = labels)
  df
#+end_src

#+RESULTS:
:              X         Y         Z
: ID_0  6.964692  2.861393  2.268515
: ID_1  5.513148  7.194690  4.231065
: ID_2  9.807642  6.848297  4.809319
: ID_3  3.921175  3.431780  7.290497
: ID_4  4.385722  0.596779  3.980443

** 距離行列で階層的クラスタリングを実行する
#+begin_src python :session :results value
  from scipy.spatial.distance import pdist, squareform
  # pdist で距離を計算、squareform で対称行列を作成
  row_dist = pd.DataFrame(squareform(pdist(df, metric = "euclidean")),
                          columns = labels, index = labels)
  row_dist
#+end_src

#+RESULTS:
:           ID_0      ID_1      ID_2      ID_3      ID_4
: ID_0  0.000000  4.973534  5.516653  5.899885  3.835396
: ID_1  4.973534  0.000000  4.347073  5.104311  6.698233
: ID_2  5.516653  4.347073  0.000000  7.244262  8.316594
: ID_3  5.899885  5.104311  7.244262  0.000000  4.382864
: ID_4  3.835396  6.698233  8.316594  4.382864  0.000000

データセットのデータ点のペアごとに、特徴量X, Y, Zに基づいてユーグリッド距離が求められる。
ペアごとの距離からなる、対称行列が作成される。

完全連結法に基づく凝集型階層的クラスタリングを適用する
#+begin_src python :session :results output
  from scipy.cluster.hierarchy import linkage
  help(linkage)
#+end_src

#+RESULTS:
#+begin_example
Help on function linkage in module scipy.cluster.hierarchy:

linkage(y, method='single', metric='euclidean', optimal_ordering=False)
    Perform hierarchical/agglomerative clustering.

    The input y may be either a 1-D condensed distance matrix
    or a 2-D array of observation vectors.

    If y is a 1-D condensed distance matrix,
    then y must be a :math:`\binom{n}{2}` sized
    vector, where n is the number of original observations paired
    in the distance matrix. The behavior of this function is very
    similar to the MATLAB linkage function.

    A :math:`(n-1)` by 4 matrix ``Z`` is returned. At the
    :math:`i`-th iteration, clusters with indices ``Z[i, 0]`` and
    ``Z[i, 1]`` are combined to form cluster :math:`n + i`. A
    cluster with an index less than :math:`n` corresponds to one of
    the :math:`n` original observations. The distance between
    clusters ``Z[i, 0]`` and ``Z[i, 1]`` is given by ``Z[i, 2]``. The
    fourth value ``Z[i, 3]`` represents the number of original
    observations in the newly formed cluster.

    The following linkage methods are used to compute the distance
    :math:`d(s, t)` between two clusters :math:`s` and
    :math:`t`. The algorithm begins with a forest of clusters that
    have yet to be used in the hierarchy being formed. When two
    clusters :math:`s` and :math:`t` from this forest are combined
    into a single cluster :math:`u`, :math:`s` and :math:`t` are
    removed from the forest, and :math:`u` is added to the
    forest. When only one cluster remains in the forest, the algorithm
    stops, and this cluster becomes the root.

    A distance matrix is maintained at each iteration. The ``d[i,j]``
    entry corresponds to the distance between cluster :math:`i` and
    :math:`j` in the original forest.

    At each iteration, the algorithm must update the distance matrix
    to reflect the distance of the newly formed cluster u with the
    remaining clusters in the forest.

    Suppose there are :math:`|u|` original observations
    :math:`u[0], \ldots, u[|u|-1]` in cluster :math:`u` and
    :math:`|v|` original objects :math:`v[0], \ldots, v[|v|-1]` in
    cluster :math:`v`. Recall, :math:`s` and :math:`t` are
    combined to form cluster :math:`u`. Let :math:`v` be any
    remaining cluster in the forest that is not :math:`u`.

    The following are methods for calculating the distance between the
    newly formed cluster :math:`u` and each :math:`v`.

      ,* method='single' assigns

        .. math::
           d(u,v) = \min(dist(u[i],v[j]))

        for all points :math:`i` in cluster :math:`u` and
        :math:`j` in cluster :math:`v`. This is also known as the
        Nearest Point Algorithm.

      ,* method='complete' assigns

        .. math::
           d(u, v) = \max(dist(u[i],v[j]))

        for all points :math:`i` in cluster u and :math:`j` in
        cluster :math:`v`. This is also known by the Farthest Point
        Algorithm or Voor Hees Algorithm.

      ,* method='average' assigns

        .. math::
           d(u,v) = \sum_{ij} \frac{d(u[i], v[j])}
                                   {(|u|*|v|)}

        for all points :math:`i` and :math:`j` where :math:`|u|`
        and :math:`|v|` are the cardinalities of clusters :math:`u`
        and :math:`v`, respectively. This is also called the UPGMA
        algorithm.

      ,* method='weighted' assigns

        .. math::
           d(u,v) = (dist(s,v) + dist(t,v))/2

        where cluster u was formed with cluster s and t and v
        is a remaining cluster in the forest (also called WPGMA).

      ,* method='centroid' assigns

        .. math::
           dist(s,t) = ||c_s-c_t||_2

        where :math:`c_s` and :math:`c_t` are the centroids of
        clusters :math:`s` and :math:`t`, respectively. When two
        clusters :math:`s` and :math:`t` are combined into a new
        cluster :math:`u`, the new centroid is computed over all the
        original objects in clusters :math:`s` and :math:`t`. The
        distance then becomes the Euclidean distance between the
        centroid of :math:`u` and the centroid of a remaining cluster
        :math:`v` in the forest. This is also known as the UPGMC
        algorithm.

      ,* method='median' assigns :math:`d(s,t)` like the ``centroid``
        method. When two clusters :math:`s` and :math:`t` are combined
        into a new cluster :math:`u`, the average of centroids s and t
        give the new centroid :math:`u`. This is also known as the
        WPGMC algorithm.

      ,* method='ward' uses the Ward variance minimization algorithm.
        The new entry :math:`d(u,v)` is computed as follows,

        .. math::

           d(u,v) = \sqrt{\frac{|v|+|s|}
                               {T}d(v,s)^2
                        + \frac{|v|+|t|}
                               {T}d(v,t)^2
                        - \frac{|v|}
                               {T}d(s,t)^2}

        where :math:`u` is the newly joined cluster consisting of
        clusters :math:`s` and :math:`t`, :math:`v` is an unused
        cluster in the forest, :math:`T=|v|+|s|+|t|`, and
        :math:`|*|` is the cardinality of its argument. This is also
        known as the incremental algorithm.

    Warning: When the minimum distance pair in the forest is chosen, there
    may be two or more pairs with the same minimum distance. This
    implementation may choose a different minimum than the MATLAB
    version.

    Parameters
    ----------
    y : ndarray
        A condensed distance matrix. A condensed distance matrix
        is a flat array containing the upper triangular of the distance matrix.
        This is the form that ``pdist`` returns. Alternatively, a collection of
        :math:`m` observation vectors in :math:`n` dimensions may be passed as
        an :math:`m` by :math:`n` array. All elements of the condensed distance
        matrix must be finite, i.e., no NaNs or infs.
    method : str, optional
        The linkage algorithm to use. See the ``Linkage Methods`` section below
        for full descriptions.
    metric : str or function, optional
        The distance metric to use in the case that y is a collection of
        observation vectors; ignored otherwise. See the ``pdist``
        function for a list of valid distance metrics. A custom distance
        function can also be used.
    optimal_ordering : bool, optional
        If True, the linkage matrix will be reordered so that the distance
        between successive leaves is minimal. This results in a more intuitive
        tree structure when the data are visualized. defaults to False, because
        this algorithm can be slow, particularly on large datasets [2]_. See
        also the `optimal_leaf_ordering` function.

        .. versionadded:: 1.0.0

    Returns
    -------
    Z : ndarray
        The hierarchical clustering encoded as a linkage matrix.

    Notes
    -----
    1. For method 'single', an optimized algorithm based on minimum spanning
       tree is implemented. It has time complexity :math:`O(n^2)`.
       For methods 'complete', 'average', 'weighted' and 'ward', an algorithm
       called nearest-neighbors chain is implemented. It also has time
       complexity :math:`O(n^2)`.
       For other methods, a naive algorithm is implemented with :math:`O(n^3)`
       time complexity.
       All algorithms use :math:`O(n^2)` memory.
       Refer to [1]_ for details about the algorithms.
    2. Methods 'centroid', 'median', and 'ward' are correctly defined only if
       Euclidean pairwise metric is used. If `y` is passed as precomputed
       pairwise distances, then it is the user's responsibility to assure that
       these distances are in fact Euclidean, otherwise the produced result
       will be incorrect.

    See Also
    --------
    scipy.spatial.distance.pdist : pairwise distance metrics

    References
    ----------
    .. [1] Daniel Mullner, "Modern hierarchical, agglomerative clustering
           algorithms", :arXiv:`1109.2378v1`.
    .. [2] Ziv Bar-Joseph, David K. Gifford, Tommi S. Jaakkola, "Fast optimal
           leaf ordering for hierarchical clustering", 2001. Bioinformatics
           :doi:`10.1093/bioinformatics/17.suppl_1.S22`

    Examples
    --------
    >>> from scipy.cluster.hierarchy import dendrogram, linkage
    >>> from matplotlib import pyplot as plt
    >>> X = [[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]]

    >>> Z = linkage(X, 'ward')
    >>> fig = plt.figure(figsize=(25, 10))
    >>> dn = dendrogram(Z)

    >>> Z = linkage(X, 'single')
    >>> fig = plt.figure(figsize=(25, 10))
    >>> dn = dendrogram(Z)
    >>> plt.show()
#+end_example

- 正しくないアプローチ
  #+begin_src python :session :results value
    row_clusters = linkage(row_dist, method = "complete", metric = "euclidean")
    row_clusters
  #+end_src

  #+RESULTS:
  | 0 | 4 |  6.52197264 | 2 |
  | 1 | 2 |  6.72960262 | 2 |
  | 3 | 5 |  8.53924727 | 3 |
  | 6 | 7 | 12.44482356 | 5 |

- 正しいアプローチ
  #+begin_src python :session :results value
    row_clusters = linkage(pdist(df, metric = "euclidean"), method = "complete")
    row_clusters
  #+end_src

  #+RESULTS:
  | 0 | 4 | 3.83539555 | 2 |
  | 1 | 2 | 4.34707339 | 2 |
  | 3 | 5 | 5.89988504 | 3 |
  | 6 | 7 | 8.31659367 | 5 |

  #+begin_src python :session :results value
    row_clusters = linkage(df.values, method = "complete", metric = "euclidean")
    row_clusters
  #+end_src

  #+RESULTS:
  | 0 | 4 | 3.83539555 | 2 |
  | 1 | 2 | 4.34707339 | 2 |
  | 3 | 5 | 5.89988504 | 3 |
  | 6 | 7 | 8.31659367 | 5 |

これらの結果をpandasのDataFrameオブジェクトに変換する
#+begin_src python :session :results value
  pd.DataFrame(row_clusters,
               columns = ["row label 1",
                          "row label 2",
                          "distance",
                          "no. of items in clust."],
               index = ["cluster %d" % (i + 1) for i in range(row_clusters.shape[0])])
#+end_src

#+RESULTS:
:            row label 1  row label 2  distance  no. of items in clust.
: cluster 1          0.0          4.0  3.835396                     2.0
: cluster 2          1.0          2.0  4.347073                     2.0
: cluster 3          3.0          5.0  5.899885                     3.0
: cluster 4          6.0          7.0  8.316594                     5.0

連結行列
1列目と2列目は、各クラスタにおいて最も類似度が低いメンバーを示す
3列目はそれらのメンバーの距離を示す
4列目は各クラスタのメンバーの個数を示す

連結行列の結果を樹形図として表示
#+begin_src python :session :results file link
  from scipy.cluster.hierarchy import dendrogram

  plt.close("all")

  row_dendr = dendrogram(row_clusters,
                         labels = labels)
  plt.ylabel("Euclidean distance")
  plt.tight_layout()

  fname = "images/11_11.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_11.png]]

ユークリッド距離を指標とした場合、データ点ID_0とID_4、ID_1とID_2が最も類似度の高いサンプルである。

** 樹形図をヒートマップと組み合わせる
#+begin_src python :session :results file link
  plt.close("all")
  fig = plt.figure(figsize = (8, 8), facecolor = "white")
  axd = fig.add_axes([0.09, 0.1, 0.2, 0.6]) # x軸の位置、y軸の位置、幅、高さ
  row_dendr = dendrogram(row_clusters, orientation = "left")

  df_rowclust = df.iloc[row_dendr["leaves"][::-1]]

  axm = fig.add_axes([0.23, 0.1, 0.6, 0.6])
  cax = axm.matshow(df_rowclust, interpolation = "nearest", cmap = "hot_r")
  axd.set_xticks([])
  axd.set_yticks([])
  for i in axd.spines.values():
      i.set_visible(False)
  fig.colorbar(cax)
  axm.set_xticklabels([" "] + list(df_rowclust.columns))
  axm.set_yticklabels([" "] + list(df_rowclust.index))

  fname = "images/11_12.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_12.png]]

** scikit-learnを使って凝集型階層的クラスタリングを適用する
#+begin_src python :session :results output
  from sklearn.cluster import AgglomerativeClustering
  ac = AgglomerativeClustering(n_clusters = 3, # クラスタの個数
                               metric = "euclidean", # 類似度の指標（ここではユークリッド距離）
                               linkage = "complete")   # 連結方法（ここでは完全連結法）
  labels = ac.fit_predict(X)
  print("Cluster labels: %s" % labels)
#+end_src

#+RESULTS:
: Cluster labels: [1 0 0 2 1]

#+begin_src python :session :results output
  ac = AgglomerativeClustering(n_clusters = 2,
                               metric = "euclidean",
                               linkage = "complete")
  labels = ac.fit_predict(X)
  print("Cluster labels: %s" % labels)
#+end_src

#+RESULTS:
: Cluster labels: [0 1 1 0 0]

* DBSCANを使って高密度の領域を特定する
クラスタが球状であるという前提をもたない。
「コア点」「ボーダー点」「ノイズ点」

半月状の構造をもつデータセットを作成することで、
k-means法、階層的クラスタリング、DBSCANを比較する
#+begin_src python :session :results file link
  from sklearn.datasets import make_moons
  X, y = make_moons(n_samples = 200,
                    noise = 0.05,
                    random_state = 0)
  plt.close("all")
  plt.scatter(X[:, 0], X[:, 1])
  plt.tight_layout()

  fname = "images/11_14.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_14.png]]

#+begin_src python :session :results file link
  f, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3))
  km = KMeans(n_clusters = 2, random_state = 0)
  y_km = km.fit_predict(X)
#+end_src
