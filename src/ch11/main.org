*クラスタ分析 ―ラベルなしのデータ分析*

*教師なし学習法* に属するクラスタ分析について

#+begin_src emacs-lisp
  (pyvenv-activate "~/project/python_machine_learning")
#+end_src

#+RESULTS:

* k-means法を使った類似度によるオブジェクトのグループ化

** scikit-learnを使ったk-meansクラスタリング
k-means法の問題点の1つは、クラスタの個数kを指定しなければならないこと

#+begin_src python :session :results file link
  from sklearn.datasets import make_blobs

  X, y = make_blobs(n_samples = 150, # データ点の個数
                    n_features = 2,  # 特徴量の個数
                    centers = 3,     # クラスタの個数
                    cluster_std = 0.5, # クラスタ内の標準偏差
                    shuffle = True,    # データ点のシャッフル
                    random_state = 0)  # 乱数生成器の状態を指定

  import matplotlib.pyplot as plt
  plt.scatter(X[:, 0], X[:, 1], c = "white", marker = "o", edgecolor = "black", s = 50)
  plt.grid()
  plt.tight_layout()

  fname = "images/11_01.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_01.png]]

#+begin_src python :session :results output
  from sklearn.cluster import KMeans

  km = KMeans(n_clusters = 3,     # クラスタの個数
              init = "random",    # セントロイドの初期値をランダムに選択
              n_init = 10,
              max_iter = 300,     # k-meansアルゴリズム内部の最大イテレーション回数
              tol = 1e-04,        # 収束と判定するための相対的な許容誤差
              random_state = 0)   # セントロイドの初期化に用いる乱数生成器の状態
  y_km = km.fit_predict(X)        # クラスタ中心の計算と各データ点のインデックスの予測
#+end_src

#+RESULTS:

#+begin_src python :session :results file link
  plt.close("all")
  plt.scatter(X[y_km == 0, 0],
              X[y_km == 0, 1],
              s = 50,
              c = "lightgreen",
              edgecolor = "black",
              marker = "s",
              label = "Cluster 1")
  plt.scatter(X[y_km == 1, 0],
              X[y_km == 1, 1],
              s = 50,
              c = "orange",
              edgecolor = "black",
              marker = "o",
              label = "Cluster 2")
  plt.scatter(X[y_km == 2, 0],
              X[y_km == 2, 1],
              s = 50,
              c = "lightblue",
              edgecolor = "black",
              marker = "v",
              label = "Cluster 3")
  plt.scatter(km.cluster_centers_[:,0],
              km.cluster_centers_[:,1],
              s = 250,
              marker = "*",
              c = "red",
              edgecolor = "black",
              label = "Centroids")
  plt.legend(scatterpoints = 1)
  plt.grid()
  plt.tight_layout()

  fname = "images/11_02.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_02.png]]

** k-means++法を使ってセントロイドの初期値をよりスマートに設定する

** ハードクラスタリングとソフトクラスタリング
- *ハードクラスタリング* データセットのデータ点がそれぞれちょうど1つのクラスタに割り当てられるアルゴリズム
- *ソフトクラスタリング* データ点を1つ以上のクラスタに割り当てるアルゴリズム

** エルボー法を使ってクラスタの最適な個数を求める
#+begin_src python :session :results output
  print("Distortion: %.2f" % km.inertia_)
#+end_src

#+RESULTS:
: Distortion: 72.48

#+begin_src python :session :results file link
  distortions = []

  for i in range(1, 11):
      km = KMeans(
          n_clusters = i,
          init = 'k-means++',
          n_init = 10,
          max_iter = 300,
          random_state = 0)
      km.fit(X)
      distortions.append(km.inertia_)

  plt.close("all")
  plt.plot(range(1, 11), distortions, marker = "o")
  plt.xlabel("Number of clusters")
  plt.ylabel("Distortion")
  plt.tight_layout()
  fname = "images/11_03.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_03.png]]

k=3が良さそう

** シルエット図を使ってクラスタリングの性能を数値化する
*シルエット分析* はk-means法以外のクラスタリングアルゴリズムにも適用できる。
クラスタ内のデータ点がどの程度密にグループ化されているか

#+begin_src python :session :results file link
  km = KMeans(n_clusters = 3,
              init = 'k-means++',
              n_init = 10,
              max_iter = 300,
              tol = 1e-04,
              random_state = 0)
  y_km = km.fit_predict(X)

  import numpy as np
  from matplotlib import cm
  from sklearn.metrics import silhouette_samples
  cluster_labels = np.unique(y_km) # y_kmの要素の中で重複をなくす
  n_clusters = cluster_labels.shape[0] # 配列の長さを返す
  # シルエット係数を計算
  silhouette_vals = silhouette_samples(X, y_km, metric = "euclidean")
  y_ax_lower, y_ax_upper = 0, 0
  yticks = []

  plt.close("all")
  for i, c in enumerate(cluster_labels):
      c_silhouette_vals = silhouette_vals[y_km == c]
      c_silhouette_vals.sort()
      y_ax_upper += len(c_silhouette_vals)
      color = cm.jet(float(i) / n_clusters) # 色の値をセット
      plt.barh(range(y_ax_lower, y_ax_upper), # 水平の棒を描画
               c_silhouette_vals,             # 棒の幅
               height = 1.0,                  # 棒の高さ
               edgecolor = 'none',            # 棒の端の色
               color = color)                 # 棒の色
      yticks.append((y_ax_lower + y_ax_upper) / 2.) # クラスタレベルの表示位置を追加
      y_ax_lower += len(c_silhouette_vals)          # 底辺の棒の幅を追加

  silhouette_avg = np.mean(silhouette_vals) # シルエット係数の平均値
  plt.axvline(silhouette_avg, color = "red", linestyle = "--") # 係数の平均値に破線を引く
  plt.yticks(yticks, cluster_labels + 1)                       # クラスラベルを表示
  plt.ylabel("Cluster")
  plt.xlabel("Silhouette coefficient")
  plt.tight_layout()

  fname = "images/11_06.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:images/11_06.png]]

シルエット係数の値が1であれば「よいクラスタリング」を示すことになる。
このシルエット図は1から程遠い。

比較的悪いクラスタリングならどうなる？
#+begin_src python :session :results file link
  km = KMeans(n_clusters = 2,
              init = "k-means++",
              n_init = 10,
              max_iter = 300,
              tol = 1e-04,
              random_state = 0)
  y_km = km.fit_predict(X)
#+end_src
